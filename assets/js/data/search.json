[ { "title": "Mybatis的Spring插件扫描原理", "url": "/posts/mybatis-spring-scanner/", "categories": "Spring", "tags": "Java, Spring, Mybatis", "date": "2022-09-12 11:16:17 +0800", "snippet": " 本文主要针对Mybatis的Spring插件的扫描进行分析，从MapperScan注解切入，进行源码层面的简要解读。一、@MapperScan注解 @MapperScan是Mybatis的Spring插件的核心扫描配置，其通过Spring配置的@Import注解注册其核心配置。 package org.mybatis.spring.annotation; @Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Import(MapperScannerRegistrar.class)@Repeatable(MapperScans.class)public @interface MapperScan { // 省略了注解成员} 其中MapperScannerRegistrar配置类是@MapperScan注册的核心配置类。 二、MapperScannerRegistrar配置类 MapperScannerRegistrar是ImportBeanDefinitionRegistrar接口的实现类，我们知道所有ImportBeanDefinitionRegistrar的实现类在Spring扫描配置类的时候会进行实例化，并执行其中的关键回调方法。 package org.springframework.context.annotation; public interface ImportBeanDefinitionRegistrar { default void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry, BeanNameGenerator importBeanNameGenerator) { registerBeanDefinitions(importingClassMetadata, registry); } default void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { }} 我们看到ImportBeanDefinitionRegistrar接口中声明的回调方法，可以传入通过@Import修饰的配置注解的元数据和容器注册器。本接口的实现类会在Spring扫描配置类的过程中被扫描，然后实例化并执行相应的回调方法，这个回调方法允许用户自定义注册Bean。 MapperScannerRegistrar类中对上述回调重写的关键逻辑在于包含一个对MapperScannerConfigurer类的Bean注册过程，同时根据注解中包含的各种参数信息填充MapperScannerConfigurer类中的各项成员属性。 package org.mybatis.spring.annotation; public class MapperScannerRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) { registerBeanDefinitions(importingClassMetadata, mapperScanAttrs, registry, generateBaseBeanName(importingClassMetadata, 0)); } } void registerBeanDefinitions(AnnotationMetadata annoMeta, AnnotationAttributes annoAttrs, BeanDefinitionRegistry registry, String beanName) { BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class); builder.addPropertyValue(\"processPropertyPlaceHolders\", true); Class&lt;? extends Annotation&gt; annotationClass = annoAttrs.getClass(\"annotationClass\"); if (!Annotation.class.equals(annotationClass)) { builder.addPropertyValue(\"annotationClass\", annotationClass); } Class&lt;?&gt; markerInterface = annoAttrs.getClass(\"markerInterface\"); if (!Class.class.equals(markerInterface)) { builder.addPropertyValue(\"markerInterface\", markerInterface); } Class&lt;? extends BeanNameGenerator&gt; generatorClass = annoAttrs.getClass(\"nameGenerator\"); if (!BeanNameGenerator.class.equals(generatorClass)) { builder.addPropertyValue(\"nameGenerator\", BeanUtils.instantiateClass(generatorClass)); } Class&lt;? extends MapperFactoryBean&gt; mapperFactoryBeanClass = annoAttrs.getClass(\"factoryBean\"); if (!MapperFactoryBean.class.equals(mapperFactoryBeanClass)) { builder.addPropertyValue(\"mapperFactoryBeanClass\", mapperFactoryBeanClass); } String sqlSessionTemplateRef = annoAttrs.getString(\"sqlSessionTemplateRef\"); if (StringUtils.hasText(sqlSessionTemplateRef)) { builder.addPropertyValue(\"sqlSessionTemplateBeanName\", annoAttrs.getString(\"sqlSessionTemplateRef\")); } String sqlSessionFactoryRef = annoAttrs.getString(\"sqlSessionFactoryRef\"); if (StringUtils.hasText(sqlSessionFactoryRef)) { builder.addPropertyValue(\"sqlSessionFactoryBeanName\", annoAttrs.getString(\"sqlSessionFactoryRef\")); } List&lt;String&gt; basePackages = new ArrayList&lt;&gt;(); basePackages.addAll( Arrays.stream(annoAttrs.getStringArray(\"value\")).filter(StringUtils::hasText).collect(Collectors.toList())); basePackages.addAll(Arrays.stream(annoAttrs.getStringArray(\"basePackages\")).filter(StringUtils::hasText) .collect(Collectors.toList())); basePackages.addAll(Arrays.stream(annoAttrs.getClassArray(\"basePackageClasses\")).map(ClassUtils::getPackageName) .collect(Collectors.toList())); if (basePackages.isEmpty()) { basePackages.add(getDefaultBasePackage(annoMeta)); } String lazyInitialization = annoAttrs.getString(\"lazyInitialization\"); if (StringUtils.hasText(lazyInitialization)) { builder.addPropertyValue(\"lazyInitialization\", lazyInitialization); } builder.addPropertyValue(\"basePackage\", StringUtils.collectionToCommaDelimitedString(basePackages)); registry.registerBeanDefinition(beanName, builder.getBeanDefinition()); } private static String generateBaseBeanName(AnnotationMetadata importingClassMetadata, int index) { return importingClassMetadata.getClassName() + \"#\" + MapperScannerRegistrar.class.getSimpleName() + \"#\" + index; } private static String getDefaultBasePackage(AnnotationMetadata importingClassMetadata) { return ClassUtils.getPackageName(importingClassMetadata.getClassName()); } } 三、MapperScannerConfigurer配置类 我们知道Bean工厂后置处理器是Spring初始化全流程中的关键类，其中Bean的扫描注册等工作都需要依靠内置的Bean工厂后置处理器来实现的。MapperScannerConfigurer配置类就是是BeanDefinitionRegistryPostProcessor接口的实现类，而BeanDefinitionRegistryPostProcessor接口作为BeanFactoryPostProcessor的子接口，也是一种特别的Bean工厂后置处理器，可以实现一些自定义Bean的动态注册。 @FunctionalInterfacepublic interface BeanFactoryPostProcessor { void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;} public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor { void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException;} MapperScannerConfigurer配置类在Bean工厂后置处理器的关键回调中，实例化了一个ClassPathMapperScanner扫描类，用来扫描所有符合Mybatis映射器（mapper）规范的接口。 package org.mybatis.spring.mapper; public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware { @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) { if (this.processPropertyPlaceHolders) { processPropertyPlaceHolders(); } ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.setMapperFactoryBeanClass(this.mapperFactoryBeanClass); if (StringUtils.hasText(lazyInitialization)) { scanner.setLazyInitialization(Boolean.valueOf(lazyInitialization)); } scanner.registerFilters(); scanner.scan( StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS)); }} 四、ClassPathMapperScanner扫描类 ClassPathMapperScanner扫描类是ClassPathBeanDefinitionScannerSpring内置的class路径Bean定义扫描器的子类实现，该类重写了成员筛选isCandidateComponent方法和具体的扫描逻辑doScan方法。 package org.mybatis.spring.mapper; public class ClassPathMapperScanner extends ClassPathBeanDefinitionScanner { private Class&lt;? extends MapperFactoryBean&gt; mapperFactoryBeanClass = MapperFactoryBean.class; @Override public Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) { Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) { LOGGER.warn(() -&gt; \"No MyBatis mapper was found in '\" + Arrays.toString(basePackages) + \"' package. Please check your configuration.\"); } else { processBeanDefinitions(beanDefinitions); } return beanDefinitions; } private void processBeanDefinitions(Set&lt;BeanDefinitionHolder&gt; beanDefinitions) { GenericBeanDefinition definition; for (BeanDefinitionHolder holder : beanDefinitions) { definition = (GenericBeanDefinition) holder.getBeanDefinition(); String beanClassName = definition.getBeanClassName(); LOGGER.debug(() -&gt; \"Creating MapperFactoryBean with name '\" + holder.getBeanName() + \"' and '\" + beanClassName + \"' mapperInterface\"); // the mapper interface is the original class of the bean // but, the actual class of the bean is MapperFactoryBean definition.getConstructorArgumentValues().addGenericArgumentValue(beanClassName); // issue #59 definition.setBeanClass(this.mapperFactoryBeanClass); definition.getPropertyValues().add(\"addToConfig\", this.addToConfig); boolean explicitFactoryUsed = false; if (StringUtils.hasText(this.sqlSessionFactoryBeanName)) { definition.getPropertyValues().add(\"sqlSessionFactory\", new RuntimeBeanReference(this.sqlSessionFactoryBeanName)); explicitFactoryUsed = true; } else if (this.sqlSessionFactory != null) { definition.getPropertyValues().add(\"sqlSessionFactory\", this.sqlSessionFactory); explicitFactoryUsed = true; } if (StringUtils.hasText(this.sqlSessionTemplateBeanName)) { if (explicitFactoryUsed) { LOGGER.warn( () -&gt; \"Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.\"); } definition.getPropertyValues().add(\"sqlSessionTemplate\", new RuntimeBeanReference(this.sqlSessionTemplateBeanName)); explicitFactoryUsed = true; } else if (this.sqlSessionTemplate != null) { if (explicitFactoryUsed) { LOGGER.warn( () -&gt; \"Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.\"); } definition.getPropertyValues().add(\"sqlSessionTemplate\", this.sqlSessionTemplate); explicitFactoryUsed = true; } if (!explicitFactoryUsed) { LOGGER.debug(() -&gt; \"Enabling autowire by type for MapperFactoryBean with name '\" + holder.getBeanName() + \"'.\"); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); } definition.setLazyInit(lazyInitialization); } } @Override protected boolean isCandidateComponent(AnnotatedBeanDefinition beanDefinition) { return beanDefinition.getMetadata().isInterface() &amp;&amp; beanDefinition.getMetadata().isIndependent(); }} isCandidateComponent方法的重写逻辑让本类仅扫描独立接口。 doScan方法的重写在讲所有符合条件的接口扫描成Bean定义的基础上，对Bean定义进行修改，设置这些mapper Bean的class为MapperFactoryBean，并设置该类中的各种成员属性值，最后将这些Bean定义注册到容器。 五、MapperFactoryBean类 MapperFactoryBean类是FactoryBean接口的实现类，同时也是SqlSessionDaoSupport类的子类，持有sqlSessionTemplate对象的引用。 MapperFactoryBean类作为工厂Bean，返回的类型是之前扫面的mapper接口类型，返回的Bean对象是通过Mybatis（sql会话接口）生产的mapper实现类。 package org.mybatis.spring.mapper; public class MapperFactoryBean&lt;T&gt; extends SqlSessionDaoSupport implements FactoryBean&lt;T&gt; { private Class&lt;T&gt; mapperInterface; private boolean addToConfig = true; public MapperFactoryBean() { // intentionally empty } public MapperFactoryBean(Class&lt;T&gt; mapperInterface) { this.mapperInterface = mapperInterface; } @Override protected void checkDaoConfig() { super.checkDaoConfig(); notNull(this.mapperInterface, \"Property 'mapperInterface' is required\"); Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig &amp;&amp; !configuration.hasMapper(this.mapperInterface)) { try { configuration.addMapper(this.mapperInterface); } catch (Exception e) { logger.error(\"Error while adding the mapper '\" + this.mapperInterface + \"' to configuration.\", e); throw new IllegalArgumentException(e); } finally { ErrorContext.instance().reset(); } } } @Override public T getObject() throws Exception { return getSqlSession().getMapper(this.mapperInterface); } @Override public Class&lt;T&gt; getObjectType() { return this.mapperInterface; } @Override public boolean isSingleton() { return true; } public void setMapperInterface(Class&lt;T&gt; mapperInterface) { this.mapperInterface = mapperInterface; } public Class&lt;T&gt; getMapperInterface() { return mapperInterface; } public void setAddToConfig(boolean addToConfig) { this.addToConfig = addToConfig; } public boolean isAddToConfig() { return addToConfig; }} 这样的话就实现了将所有的mapper实现类注册到Spring容器中了。 六、总结 Mybatis的Spring插件扫描流程总结为： 首先通过@MapperScan注解注册扫描配置信息； @MapperScan注解通过@Import注解注册MapperScannerRegistrar配置类； MapperScannerRegistrar配置类通过ImportBeanDefinitionRegistrar接口的回调方法注册MapperScannerConfigurerBean工厂后置处理器； MapperScannerConfigurer类在BeanDefinitionRegistryPostProcessor接口的回调方法中实例化ClassPathMapperScanner类并执行扫描； ClassPathMapperScanner扫描过程会将所有扫描到的mapper接口的Bean定义做修改，替换class为MapperFactoryBean，并填充MapperFactoryBean的关键属性，进而注册到Spring容器中； MapperFactoryBean类是一个工厂Bean，返回的Bean类型是之前扫描过程中设置的mapper接口类型，返回的Bean对象是通过sql会话获取到的经Mybatis反射实现的mapper接口实现类。 " }, { "title": "信号量机制", "url": "/posts/semaphore/", "categories": "Operating System", "tags": "Operating System, Java, Multiprocessing, Multithreading", "date": "2022-08-16 00:54:08 +0800", "snippet": "一、信号量概述1.概念用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作，从而很方便的实现了进程互斥、进程同步。信号量其实就是一个变量（可以是一个整数，也可以是更复杂的记录型变量），可以用一个信号量来表示系统中某种资源的数量，比如：系统中只有一台打印机，就可以设置一个初值为1的信号量。原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。原语是由关中断和开中断指令实现的。一对原语：wait(S)原语和signal(S)原语，可以把原语理解为我们自己写的函数，函数名分别为wait和signal，括号里的信号量S其实就是函数调用时传入的一个参数。wait、signal原语常简称为P、V操作（来自荷兰语proberen和verhogen）。因此，常把wait(S)、signal(S)两个操作分别写为P(S)、V(S)。2.类别 整型信号量 用一个整型变量作为信号量，表示系统中某种资源的数量。 int S = 1; // 初始化信号量, 表示某种资源的剩余数// 注意wait与signal都是通过原语实现的, 是原子性的, 这里仅仅是通过C语言模拟void wait(int S) { while (S &lt;= 0); // 如果不满足需求, 则忙等待 S = S - 1;}// 注意wait与signal都是通过原语实现的, 是原子性的, 这里仅仅是通过C语言模拟void signal(int S) { S = S + 1;} 进程调用逻辑。 wait(S); // P操作useResource(); // 使用临界资源signal(S); // V操作 由于只有单一的整型信号量，只能通过忙等待阻塞进程，不满足“让权等待”的原则。 记录型信号量 整型信号量存在“忙等”问题，因此人们又提出了“记录型信号量”，即用记录型数据结构表示的信号量。 typedef struct { int value; Struct process *L;} Semaphore;// 注意wait与signal都是通过原语实现的, 是原子性的, 这里仅仅是通过C语言模拟void wait(Semaphore S) { S.value--; if (S.value &lt; 0) { // 无法获取到资源 block(S.L); // 进程调用block原语进行自我阻塞 }}// 注意wait与signal都是通过原语实现的, 是原子性的, 这里仅仅是通过C语言模拟void signal(Semaphore S) { S.value++; if (S.value &lt;= 0) { // 如果当前进程释放资源之后value仍不大于0, 说明有其他进程在排队 wakeup(S.L); // 调用wakeup原语唤醒等待队列中的第一个进程 }} 可以解决让权等待的问题；可以实现进程互斥和进程同步。 二、信号量机制实现进程互斥、同步与前驱1.进程互斥进程互斥可以通过mutex信号量的P、V操作将临界区包裹来实现。Semaphore mutex = new Semaphore(1);void process { wait(mutex); criticalArea(); signal(mutex);}2.进程同步信号量初始化为0，需要先执行的进程代码执行完之后执行信号量V操作，后执行进程代码执行之前调用信号量P操作。Semaphore s = new Semaphore(0);void processA { // 进程A临界区先执行 criticalAreaA(); signal(s);}void processB { wait(s); // 进程B临界区后执行 criticalAreaB();}3.进程前驱实际上是多个进程形成有向无环图，产生一个具有先后关系多进程同步情况。可以为每一对前驱关系都按照进程同步的方式设计一个信号量。三、信号量机制解决经典进程问题 以下例子均通过Java多线程的方式模拟多进程1.单生产者-消费者问题有一类生产者生产一种产品，一类消费者消费同一种产品。生产者将生产的产品放入一个有限缓冲区，消费者从这个缓冲区内取走产品消费。现在需要保证各方对缓冲区的操作是互斥的，同时缓冲区满了则暂停生产，缓冲区空了则暂停消费。解法：增删缓冲区的操作作为临界区互斥，同时将缓冲区空位的个数和产品的个数都作为资源信号量引导进程同步。package com.katus;import lombok.extern.slf4j.Slf4j;import java.util.Random;import java.util.concurrent.Semaphore;/** * 通过信号量机制实现的单生产者-消费者模型 * * @author SUN Katus * @version 1.0, 2022-08-12 */@Slf4jpublic class SingleProducerConsumer { private static final int CAPACITY = 5, BOUND = 20; private final Random random; private final Buffer buffer; private final Semaphore mutex, full, empty; public SingleProducerConsumer() { this.random = new Random(); this.buffer = new Buffer(); this.mutex = new Semaphore(1); this.full = new Semaphore(0); this.empty = new Semaphore(CAPACITY); } public class Producer implements Runnable { @Override public void run() { while (true) { try { int product = random.nextInt(BOUND); Thread.sleep(100); // 互斥操作必须在同步操作之后, 否则会死锁 (保证只有满足了同步关系才允许访问临界资源) empty.acquire(); mutex.acquire(); buffer.write(product); log.info(\"Produced a product: {}\", product); mutex.release(); full.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class Consumer implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); full.acquire(); mutex.acquire(); int product = buffer.read(); log.info(\"Consumed a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static class Buffer { private final int[] buffer; private int index; public Buffer() { this.buffer = new int[CAPACITY]; this.index = 0; } public boolean isFull() { return index == CAPACITY; } public boolean isEmpty() { return index == 0; } public void write(int product) { buffer[index++] = product; } public int read() { return buffer[--index]; } } public static void main(String[] args) { SingleProducerConsumer singleProducerConsumer = new SingleProducerConsumer(); new Thread(singleProducerConsumer.new Consumer(), \"Consumer\").start(); new Thread(singleProducerConsumer.new Producer(), \"Producer\").start(); }}2.多生产者-消费者问题多类生产者消费者生产、消费不同的产品，但是共享同一个有限缓冲区。现在需要保证各方对缓冲区的操作是互斥的，同时能够满足各类生产者消费者正常生产消费。解法：增删缓冲区的操作作为临界区互斥，同时将缓冲区空位的个数和各类产品的个数都作为资源信号量引导进程同步。package com.katus;import lombok.ToString;import lombok.extern.slf4j.Slf4j;import java.util.HashSet;import java.util.Set;import java.util.concurrent.Semaphore;/** * 通过信号量机制实现的多类别生产者-消费者模型 * * @author SUN Katus * @version 1.0, 2022-08-12 */@Slf4jpublic class MultiProducerConsumer { private static final int CAPACITY = 5, BOUND = 20; private final Buffer buffer; private final Semaphore mutex, a, b, empty; public MultiProducerConsumer() { this.buffer = new Buffer(); this.mutex = new Semaphore(1); this.a = new Semaphore(0); this.b = new Semaphore(0); this.empty = new Semaphore(CAPACITY); } public class ProducerA implements Runnable { @Override public void run() { int i = 0; while (true) { try { Thread.sleep(100); Product product = new ProductA(i); i = (i + 1) % BOUND; empty.acquire(); mutex.acquire(); buffer.write(product); log.info(\"Produced a product: {}\", product); mutex.release(); a.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class ProducerB implements Runnable { @Override public void run() { int i = 0; while (true) { try { Thread.sleep(100); Product product = new ProductB(i); i = (i + 1) % BOUND; empty.acquire(); mutex.acquire(); buffer.write(product); log.info(\"Produced a product: {}\", product); mutex.release(); b.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class ConsumerA implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); a.acquire(); mutex.acquire(); Product product = buffer.readByType(\"A\"); log.info(\"Consumed a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class ConsumerB implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); b.acquire(); mutex.acquire(); Product product = buffer.readByType(\"B\"); log.info(\"Consumed a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static class Buffer { private final Set&lt;Product&gt; set; public Buffer() { this.set = new HashSet&lt;&gt;(); } public void write(Product product) { set.add(product); } public Product readByType(String productName) { for (Product product : set) { if (productName.equals(product.getName())) { set.remove(product); return product; } } return null; } } public static abstract class Product { public abstract String getName(); } @ToString public static class ProductA extends Product { private final int id; public ProductA(int id) { this.id = id; } @Override public String getName() { return \"A\"; } } @ToString public static class ProductB extends Product { private final int id; public ProductB(int id) { this.id = id; } @Override public String getName() { return \"B\"; } } public static void main(String[] args) { MultiProducerConsumer multiProducerConsumer = new MultiProducerConsumer(); new Thread(multiProducerConsumer.new ProducerA(), \"ProducerA\").start(); new Thread(multiProducerConsumer.new ProducerB(), \"ProducerB\").start(); new Thread(multiProducerConsumer.new ConsumerA(), \"ConsumerA\").start(); new Thread(multiProducerConsumer.new ConsumerB(), \"ConsumerB\").start(); }}3.吸烟者问题一类生产者（提供者）生产多种产品，多类消费者（吸烟者）消费各自的产品类型，使用同一个共享缓冲区。需要保证各方对缓冲区的操作是互斥的，同时能够满足生产者和各类消费者正常生产消费。解法：增删缓冲区的操作作为临界区互斥，同时将缓冲区空位的个数和各类产品的个数都作为资源信号量引导进程同步。package com.katus;import lombok.ToString;import lombok.extern.slf4j.Slf4j;import java.util.HashSet;import java.util.Random;import java.util.Set;import java.util.concurrent.Semaphore;/** * 通过信号量解决吸烟者问题(单生产者生产多种产品, 多种消费者消费不同的产品) * * @author SUN Katus * @version 1.0, 2022-08-12 */@Slf4jpublic class ProviderSmoker { private static final int CAPACITY = 5, BOUND = 20; private final Buffer buffer; private final Semaphore mutex, a, b, c, empty; public ProviderSmoker() { this.buffer = new Buffer(); this.mutex = new Semaphore(1); this.a = new Semaphore(0); this.b = new Semaphore(0); this.c = new Semaphore(0); this.empty = new Semaphore(CAPACITY); } public class Provider implements Runnable { private final Random random; public Provider() { this.random = new Random(); } @Override public void run() { int i = 0; while (true) { try { Thread.sleep(50); int r = random.nextInt(3); Product product; switch (r) { case 0: product = new ProductA(i); break; case 1: product = new ProductB(i); break; default: product = new ProductC(i); } i = (i + 1) % BOUND; empty.acquire(); mutex.acquire(); buffer.write(product); log.info(\"Provide a product: {}\", product); mutex.release(); switch (r) { case 0: a.release(); break; case 1: b.release(); break; default: c.release(); } } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class SmokerA implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); a.acquire(); mutex.acquire(); Product product = buffer.readByType(\"A\"); log.info(\"Smoke a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class SmokerB implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); b.acquire(); mutex.acquire(); Product product = buffer.readByType(\"B\"); log.info(\"Smoke a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class SmokerC implements Runnable { @Override public void run() { while (true) { try { Thread.sleep(100); c.acquire(); mutex.acquire(); Product product = buffer.readByType(\"C\"); log.info(\"Smoke a product: {}\", product); mutex.release(); empty.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static class Buffer { private final Set&lt;Product&gt; set; public Buffer() { this.set = new HashSet&lt;&gt;(); } public void write(Product product) { set.add(product); } public Product readByType(String productName) { for (Product product : set) { if (productName.equals(product.getName())) { set.remove(product); return product; } } return null; } } public static abstract class Product { public abstract String getName(); } @ToString public static class ProductA extends Product { private final int id; public ProductA(int id) { this.id = id; } @Override public String getName() { return \"A\"; } } @ToString public static class ProductB extends Product { private final int id; public ProductB(int id) { this.id = id; } @Override public String getName() { return \"B\"; } } @ToString public static class ProductC extends Product { private final int id; public ProductC(int id) { this.id = id; } @Override public String getName() { return \"C\"; } } public static void main(String[] args) { ProviderSmoker providerSmoker = new ProviderSmoker(); new Thread(providerSmoker.new Provider(), \"Provider\").start(); new Thread(providerSmoker.new SmokerA(), \"SmokerA\").start(); new Thread(providerSmoker.new SmokerB(), \"SmokerB\").start(); new Thread(providerSmoker.new SmokerC(), \"SmokerC\").start(); }}4.读者-写者问题多个读写进程对同一文件资源进行读写，需要保证读进程之间可以并发执行，而写进程只能单独执行。解法：通过一个整型变量记录当前有多少个读进程正在读，一个互斥信号量保证对前述整型变量的操作是原子性的；一个读写信号量表示读写资源只能被各种读进程或者一个写进程占用，一个写信号量保证写进程不会饿死。package com.katus;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.Semaphore;/** * 用信号量解决读者-写者问题 * * @author SUN Katus * @version 1.0, 2022-08-12 */@Slf4jpublic class ReaderWriter { private static final int BOUND = 20; /** * 保证记录读进程数量的操作互斥(保证对readCount的操作是原子化的, 当然也可以使用原子类) */ private final Semaphore mutex; /** * 保证写进程和其他进程之间的互斥关系 */ private final Semaphore rw; /** * 保证写进程不会饿死 */ private final Semaphore w; /** * 读进程正在读的数量 */ private int readCount; public ReaderWriter() { this.mutex = new Semaphore(1); this.rw = new Semaphore(1); this.w = new Semaphore(1); this.readCount = 0; } public class Reader implements Runnable { @Override public void run() { int i = 0; while (true) { try { Thread.sleep(50); w.acquire(); mutex.acquire(); if (readCount == 0) { rw.acquire(); } readCount++; mutex.release(); w.release(); log.info(\"Reading {} ...\", i); i = (i + 1) % BOUND; mutex.acquire(); if (readCount == 1) { rw.release(); } readCount--; mutex.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public class Writer implements Runnable { @Override public void run() { int i = 0; while (true) { try { Thread.sleep(100); w.acquire(); rw.acquire(); log.info(\"Writing {} ...\", i); i = (i + 1) % BOUND; rw.release(); w.release(); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static void main(String[] args) { ReaderWriter readerWriter = new ReaderWriter(); new Thread(readerWriter.new Reader(), \"Reader\").start(); new Thread(readerWriter.new Writer(), \"Writer\").start(); }}5.哲学家进餐问题五个哲学家围着一个圆桌吃饭，总计五支筷子，分别放在每个哲学家之间。每个哲学家只能思考或者吃饭，吃饭需要同时拿取左右两边的筷子。解法：规定哲学家获取筷子的顺序，比如要求偶数号哲学家先拿左筷子，而奇数号哲学家先拿右筷子。package com.katus;import lombok.ToString;import lombok.extern.slf4j.Slf4j;import java.util.Random;import java.util.concurrent.Semaphore;/** * 用信号量解决哲学家进餐问题 (预防死锁) * * 策略1 总计N个哲学家, 最多只允许N-1个哲学家同时吃饭 * * 策略2 要求偶数编号哲学家和奇数编号哲学家拿筷子的顺序相反 (本实现) * * 策略3 将哲学家拿筷子的过程互斥 * * @author SUN Katus * @version 1.0, 2022-08-12 */@Slf4jpublic class PhilosopherMeal { private static final int CAPACITY = 5; private final Semaphore[] chopsticks; private final Random random; public PhilosopherMeal() { this.chopsticks = new Semaphore[CAPACITY]; for (int i = 0; i &lt; CAPACITY; i++) { chopsticks[i] = new Semaphore(1); } this.random = new Random(); } @ToString public class Philosopher implements Runnable { private final int id; public Philosopher(int id) { this.id = id; } @Override public void run() { while (true) { try { Thread.sleep(100); boolean meal = random.nextBoolean(); if (meal) { int rightId = (id + 1) % CAPACITY; if (id % 2 == 0) { chopsticks[id].acquire(); chopsticks[rightId].acquire(); } else { chopsticks[rightId].acquire(); chopsticks[id].acquire(); } log.info(\"{} is eating...\", this); chopsticks[id].release(); chopsticks[rightId].release(); } else { log.info(\"{} is thinking...\", this); } } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static void main(String[] args) { PhilosopherMeal philosopherMeal = new PhilosopherMeal(); for (int i = 0; i &lt; CAPACITY; i++) { new Thread(philosopherMeal.new Philosopher(i), \"Philosopher-\" + i).start(); } }}" }, { "title": "Linux服务器多网卡多网关配置解决方案", "url": "/posts/linux-multi-gateway-config/", "categories": "Operations", "tags": "Linux, Computer Network", "date": "2021-08-19 11:14:51 +0800", "snippet": " 本文为本人尝试修复实验室Linux服务器网络的过程中的经验总结，如有错误，还望不吝赐教。一、网络情况软硬件条件硬件条件 服务器四台 每台服务器有万兆网卡2块（光纤口）、千兆网卡2块（RJ45） 交换机、路由器、网线若干 入户网线1条软件条件 服务器操作系统：CentOS 8 入户网线可以通过L2TP登录校园网进而访问互联网 校园网内的固定IP四个预期效果 四台服务器之间通过万兆网卡组合局域网，网段192.168.1.0\\24 四台服务器的其中一个千兆网卡各持有一个校园网固定IP，用于远程访问，网段10.79.224.0/21 四台服务器均需要访问互联网，由于服务器本身的L2TP拨号需要占用四个上网账号且不稳定，计划将L2TP的上网账号配置在路由器，形成新的局域网，进而上网，局域网网段192.168.10.0/24二、网络拓扑网络拓扑图注意事项 路由器需要支持L2TP拨号功能，而且将校园网上网账号配置在其中。 路由器需要支持DHCP分配IP时，将指定IP与指定的MAC地址绑定的功能，方便后期的服务器网卡配置。 虽然采用无线路由器，但是实际上不使用其无线功能，均采用有线连接，以保证稳定性和安全性。如果路由器的LAN口有限，可以续接交换机，对网络拓扑没有影响。三、服务器配置 针对每一台服务器的配置均相同，仅有IP地址不同，以td0服务器的配置为例。所有的配置均使用root用户进行为宜。1.路由配置 查看路由 # 配置前通过route命令查看路由, 仅增加缺失的路由route 添加路由 # 万兆光纤内网route add -net 192.168.1.0 netmask 255.255.255.0 dev eno1np0# 千兆内网route add -net 192.168.10.0 netmask 255.255.255.0 dev eno3# 千兆外网route add -net 10.79.224.0 netmask 255.255.248.0 dev eno4# 默认网关route add default gw 192.168.10.1 dev eno3 路由配置结果 需要保证图片中default、10.79.224.0、192.168.1.0、192.168.10.0四行内容。172.17.0.0是Docker容器的网络，无需关心。192.168.122.0是服务器维护的专用网络，也无需修改。2.路由表配置 新增路由表 # 新增lan_q与wan_q两个路由表echo \"101 lan_q\" &gt;&gt; /etc/iproute2/rt_tablesecho \"102 wan_q\" &gt;&gt; /etc/iproute2/rt_tables# 也可以使用vi命令进行文件编辑 配置路由表内容 # 定义路由表规则ip route add default via 192.168.10.1 dev eno3 table lan_qip route add default via 10.79.231.254 dev eno4 table wan_q 配置IP规则 # 不同的IP访问流量走不同的路由表(路由规则)ip rule add from 192.168.10.3 table lan_qip rule add from 10.79.231.83 table wan_q 3.其他必要命令（非配置内容）# 查看指定路由表的路由规则ip route list table &lt;table_name&gt;# 清空指定路由表的路由规则ip route flush table &lt;table_name&gt;# 查看路由和IP规则ip route showip rule show# 网络配置重载nmcli c reload 命令参考：Linux 路由表设置 之 route 指令详解4.额外配置 配置主机名（略）5.检验连通性检验ping 192.168.1.7ping 10.79.231.86ping www.baidu.comIP规则检验 在校园网环境中通过ssh直连固定IP，查看是否可以连接。四、额外说明如以本文为参考，请参照自己的网络环境中的IP分配，勿盲从。" }, { "title": "Java开发技术之多线程", "url": "/posts/java-thread/", "categories": "Java", "tags": "Java, Multithreading", "date": "2020-08-14 19:53:17 +0800", "snippet": " 暑假期间，个人对一些未来研究生阶段可能会常用的编程技术进行重新一轮的系统复习和学习，及希望能够查缺补漏，有所提升。本文也是作为复习和学习过程中的笔记，用于长久的记录。不排除其中可能含有部分疏漏和错误，如有发现，希望各位能够批评指正，谢谢。一、多线程概述（一）概述 每个进程有独立的方法区、堆空间、虚拟机栈和程序计数器。 一个进程可能拥有多个线程。 每个线程有独立的虚拟机栈和程序计数器，但是一个进程下的多线程共享方法区和堆空间。（二）线程的优先级 线程存在优先级，优先级越高，CPU将资源分配到该线程的可能性越大，但是这并不意味着高优先级的线程一定先运行。 Java线程优先级分为十个级别，从1~10（整数），默认是5。 最低优先级Thread.MIN_PRIORITY（1） 默认优先级Thread.NORM_PRIORITY（5） 最高优先级Thread.MAX_PRIORITY（10） 二、线程的生命周期 线程的状态分为五种。 新建：完成线程创建。 就绪：线程可以随时开始运行，但是尚未处于运行状态。 运行：线程正在运行。 死亡：线程完成运行。 阻塞：线程被限制无法运行，只能通过其他命令使其重新进入就绪/运行状态。 三、创建多线程的方法（一）创建Thread子类 创建Thread子类，并重写run()方法。package com.katus.thread;/** * 创建多线程的方式 1: 创建Thread子类 * @author katus * @version 1.0, 2020-08-09 */class MyThread extends Thread { // 因为要求继承Thread类 导致本类不能是其他类的子类 public MyThread(String name) { super(name); } @Override public void run() { // 此线程执行的操作 try { sleep(1000); // 本线程阻塞1000ms } catch (InterruptedException e) { e.printStackTrace(); } for (int i = 0; i &lt; 50; i++) { if (i % 2 == 0) { System.out.println(this.getName() + \":\" + i); }// if (i % 20 == 0) {// Thread.yield(); // 释放当前线程CPU的执行权, 但是有可能又被CPU分配到了// } } }}public class ThreadTest { public static void main(String[] args) { Thread.currentThread().setName(\"Thread-main\"); Thread.currentThread().setPriority(Thread.MIN_PRIORITY); // 设置最低优先级 // 创建线程对象 MyThread t1 = new MyThread(\"Thread-1st\"); t1.setPriority(Thread.MAX_PRIORITY); // 设置最高优先级 // 开始执行线程, 主线程继续 // t1.setName(\"Thread-1st\"); t1.start(); // 启动当前线程并调用当前线程的run() // t1.run(); // 错误的启动线程的方法 // t1.start(); // 不能让已经start的线程再次start for (int i = 0; i &lt; 50; i++) { if (i % 2 == 0) { System.out.println(Thread.currentThread().getName() + \":\" + i); } if (i == 30) { try { t1.join(); // 调用后本线程阻塞, t1线程执行至结束, 本线程取消阻塞 } catch (InterruptedException e) { e.printStackTrace(); } } } System.out.println(t1.isAlive()); // 判断线程是否存活 如果执行完毕即不存活 }}（二）创建Runnable接口的实现类 创建一个类实现Runnable接口，重写其中的run()方法。 本方法的优势在于创建的类可以作为其他有意义自定义类的子类，（相对于上一种方法）不必受单继承的限制。package com.katus.thread;/** * 创建多线程的方式 2: 实现Runnable接口 * @author katus * @version 1.0, 2020-08-10 */class MThread implements Runnable { // 实现Runnable接口中的run方法 @Override public void run() { for (int i = 0; i &lt; 100; i++) { if (i % 2 == 0) { System.out.println(i); } } }}public class ThreadTest1 { public static void main(String[] args) { MThread mThread = new MThread(); // 创建实现类对象 Thread t1 = new Thread(mThread); // 构造线程 t1.start(); // 启动线程 // 再次启动一个线程 Thread t2 = new Thread(mThread); t2.start(); }}（三）创建Callable接口的实现类 创建一个类，实现Callable接口，重写其中的call()方法。 该方法要求Java 5以上版本。 本方法的优势在于创建的线程允许抛出异常，允许有返回值，同时支持泛型。package com.katus.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;/** * 创建多线程的方式 3: 实现Callable接口 (Java5 新特性) * @author katus * @version 1.0, 2020-08-10 */// 创建Callable接口的实现类class NumThread implements Callable { // 重写call方法 内部为线程的操作 @Override public Object call() throws Exception { // 优势：可以抛出异常 可以有返回值 支持泛型 int sum = 0; for (int i = 0; i &lt;= 100; i++) { if (i % 2 == 0) { System.out.println(i); sum += i; } } return sum; }}public class ThreadTest2 { public static void main(String[] args) { // 创建Callable实现类的对象 NumThread numThread = new NumThread(); // 通过Callable实现类对象构建FutureTask类的对象 FutureTask futureTask = new FutureTask(numThread); // 通过FutureTask类对象构建Thread类的对象 即构建线程 new Thread(futureTask).start(); try { // 下方法的返回值即为Callable实现类中call方法的返回值 Object sum = futureTask.get(); System.out.println(\"sum = \" + sum); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } }}（四）使用线程池 通过Executors类创建线程池，设定线程池参数，通过线程池提交/执行多线程。 该方法要求Java 5以上版本。 优势在于方便进行线程管理和调度。package com.katus.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.ThreadPoolExecutor;/** * 创建多线程的方式 4: 使用线程池 (Java5 新特性) * @author katus * @version 1.0, 2020-08-10 */class NumberThread implements Runnable { @Override public void run() { for (int i = 0; i &lt;= 100; i++) { if (i % 2 == 0) { System.out.println(Thread.currentThread().getName() + \":\" + i); } } }}class NumberThread1 implements Callable { @Override public Object call() throws Exception { for (int i = 0; i &lt;= 100; i++) { if (i % 2 != 0) { System.out.println(Thread.currentThread().getName() + \":\" + i); } } return null; }}public class ThreadTest3 { public static void main(String[] args) { // 提供指定线程数量的线程池 ExecutorService service = Executors.newFixedThreadPool(10); // ExecutorService 是返回对象实现的接口 System.out.println(service.getClass()); // 验证实际返回的对象的类 // 可选设置线程池的属性 ThreadPoolExecutor executor = (ThreadPoolExecutor) service; // ThreadPoolExecutor 才是实际返回的对象 executor.setCorePoolSize(15); executor.setMaximumPoolSize(12); // executor.setKeepAliveTime(...); NumberThread numberThread = new NumberThread(); service.execute(numberThread); // 接受Runnable实现类 NumberThread1 numberThread1 = new NumberThread1(); service.submit(numberThread1); // 接受Callable/Runnable实现类 // 关闭线程池 service.shutdown(); }}四、线程安全（一）同步代码块 一个被synchronized关键字修饰的代码块，需要传入一个Object对象作为同步监视器，该Object对象作为是否需要限制的标志，即需要同一个同步监视器的代码块中的代码同时只能有一个线程在运行。 同步代码块执行完成之后，相应的同步监视器会被释放。package com.katus.thread;/** * 线程安全之同步代码块 * @author katus * @version 1.0, 2020-08-10 */class TicketSaleWindow implements Runnable { private int ticket; private final Object lock = new Object(); public TicketSaleWindow(int number) { ticket = number; } @Override public void run() { while (true) { // 同步代码块 接受同步监视器（锁）任何类的对象 要求多线程的锁是同一个对象 synchronized (lock) { // synchronized (this) 最简单的写法 // synchronized (TicketSaleWindow.class) if (ticket &gt; 0) { System.out.println(Thread.currentThread().getName() + \":\" + ticket--); } else { break; } } } }}public class TicketSale { public static void main(String[] args) { TicketSaleWindow window = new TicketSaleWindow(250); Thread t1 = new Thread(window), t2 = new Thread(window), t3 = new Thread(window); t1.start(); t2.start(); t3.start(); }}（二）同步方法 同步方法就是具有同步功能的方法，相当于整个方法体都是同步的，方法声明被synchronized关键字修饰。 同步方法也是需要同步监视器的，但是是隐性的。 静态同步方法，同步监视器是类名.class。 非静态同步方法，同步监视器是this。 同步方法体执行完成之后会释放同步监视器。package com.katus.thread;/** * 线程安全之同步方法 本质上就是把同步代码块变成了整个方法 * @author katus * @version 1.0, 2020-08-10 */class TicketSaleWindow2 implements Runnable { private int ticket; public TicketSaleWindow2(int number) { ticket = number; } @Override public void run() { do { show(); } while (ticket &gt; 0); } /*要注意同步方法在继承Thread类的方式下要改写成静态方法才能获得线程安全!!! 同步监视器为TicketSaleWindow2.class*/ private synchronized void show() { // 非静态同步方法的默认监视器是this if (ticket &gt; 0) { System.out.println(Thread.currentThread().getName() + \":\" + ticket--); } }}public class TicketSale2 { public static void main(String[] args) { TicketSaleWindow2 window = new TicketSaleWindow2(250); Thread t1 = new Thread(window), t2 = new Thread(window), t3 = new Thread(window); t1.start(); t2.start(); t3.start(); }}（三）AQS框架实现类（API锁） ReentrantLock类对象，顾名思义，其lock()方法和unlock()方法成对使用，lock()方法调用后，其他线程都无法执行，只有等unlock()方法调用之后才会解锁。 该方法要求Java 5以上版本。package com.katus.thread;import java.util.concurrent.locks.ReentrantLock;/** * 线程安全之Lock锁 (Java5新特性) * @author katus * @version 1.0, 2020-08-10 */class TicketSaleWindow3 implements Runnable { private int ticket; private final ReentrantLock lock = new ReentrantLock(); public TicketSaleWindow3(int number) { ticket = number; } @Override public void run() { while (true) { try { lock.lock(); if (ticket &gt; 0) { System.out.println(Thread.currentThread().getName() + \":\" + ticket--); } else { break; } } finally { lock.unlock(); } } }}public class LockTest {}五、线程通信 notify() notifyAll() wait() 只能用在同步方法和同步代码块中。 三个方法的调用者必须是当前同步代码块或者同步方法的同步监视器。 三个方法是定义在Object类中的所有的对象均有资格调用。package com.katus.thread;/** * 线程通信 * @author katus * @version 1.0, 2020-08-10 */class NumberPainter implements Runnable { private int number; public NumberPainter() { this(0); } public NumberPainter(int number) { this.number = number; } @Override public void run() { while (true) { synchronized (this) { // 一旦执行此方法 会有一个wait中的阻塞线程被唤醒(选取其中优先级高的线程) notify(); // this.notify(); // 一旦执行此方法 全部wait中的阻塞线程被唤醒 // notifyAll(); if (number &lt;= 100) { System.out.println(Thread.currentThread().getName() + \":\" + number++); } else { break; } try { // 一旦执行此方法 本线程立刻阻塞 同时释放同步监视器 wait(); // this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } }}public class ThreadCommunicationTest { public static void main(String[] args) { NumberPainter numberPainter = new NumberPainter(); Thread t1 = new Thread(numberPainter), t2 = new Thread(numberPainter); t1.start(); t2.start(); }}" }, { "title": "Java基础开发技术查缺补漏笔记", "url": "/posts/java-adv/", "categories": "Java", "tags": "Java", "date": "2020-08-12 21:08:07 +0800", "snippet": " 暑假期间，个人对一些未来研究生阶段可能会常用的编程技术进行重新一轮的系统复习和学习，及希望能够查缺补漏，有所提升。本文也是作为复习和学习过程中的笔记，用于长久的记录。不排除其中可能含有部分疏漏和错误，如有发现，希望各位能够批评指正，谢谢。一、准备工作 JDK版本：1.8.0_261（Java HotSpot TM）（一）Java 文档注释/** * @auther katus * @version v1.0 * 这是一个文档注释，是Java所特有的，可以被javadoc所解析。 */文档注释可以通过javadoc命令直接生成到技术文档中。javadoc -d dirname -author -version xxx.java二、Java 基本语法Java代码采用Unicode编码，因此可以使用任意Unicode编码作为标识符，哪怕是中文。class Student { String 姓名 = \"katus\"; int 学号 = 29;}自动类型转换：byte、short、char三种类型之间的运算的结果为int类型。Integer类中含有进制转化的方法。取模运算符的结果符号与被模数相同。赋值运算符不会改变操作数的数据类型。short s1 = 2;//s1 = s1 + 2; 编译错误s1 += 2; //编译通过 4s1 *= 0.5; //编译通过 2移位操作，»右移的部位数取决于原始的符号位，»&gt;永远用0补位。三、数组数组的相关操作类可以参照Arrays类。四、面向对象编程（一）匿名对象的使用new Phone().sendEmail();setPhone(new Phone());（二）可变个数形参Java5新特性，可变参数必须是参数表的最后一个参数，且只能有一个。public void method(String ... strs) { //可以匹配0至多个字符串 for(String str : strs) { //todo }}JavaBean，满足类是公共类，含无参数公共构造器，有属性且有对应的get和set方法。多构造器嵌套调用的时候需要保证调用在其他操作之前。（三）继承中重写方法 重写的方法的权限修饰符必须不小于父类中被重写的方法，而且不能重写其中private的方法（实际上是一个新的方法）。 重写方法的返回值类型必须与被重写方法返回值类型相同（基本数据类型等）或者其子类（仅引用数据类型）。 重写方法抛出的异常不能超过被重写方法中声明的抛出的异常。 只能重写非静态方法。通过子类构造器构造对象时，默认先构造父类对象。（四）多态性Person p1 = new Man();Person p2 = new Woman();多态性仅适用于方法，不适用于属性。对象引用直接输出相当于调用其toString()方法。（五）包装类Integer in1 = new Integer(12);int i1 = 12;int x = in1.intValue(); // xxxValue()int y = in1; // 自动拆箱 Java5新特性Integer in2 = i1; // 自动装箱 Java5新特性（六）String与基本数据类型之间的转化int x = 0;float y = 12.3f;String str1 = x + \"\";String str2 = String.valueOf(y); // valueOf()int x1 = Integer.parseInt(str1); // parseXxx()三目运算符要求冒号两侧的运算结果地位一致。Object o1 = true ? new Integer(1) : new Double(2.0);System.out.println(o1); // 1.0使用自动装箱的Integer对象如果范围在-128~127内，则会使用缓存中已存在的对象，不会new，==比较时为同一个。（七）单例设计模式 私有化构造器。 内部创建类的对象，且为静态。 提供公共的静态对象，返回类的对象。（八）代码块 代码块只能选择使用static关键字来修饰。 多个代码块的执行顺序完全依照声明顺序，但是静态代码块一定优先于非静态代码块。 静态代码块 随着类的加载而执行 可以初始化类的属性（静态属性） 非静态代码块 随着对象的创建而执行，而且是每创建一个对象都会执行一次。 可以在创建对象时对对象的属性进行初始化。 执行顺序：静态代码块 优先 非静态代码块 优先 构造器 // 由父及子 静态先行属性赋值的先后顺序：默认初始化 - 显式初始化 / 代码块 - 构造器 - 通过对象的set方法等。（显式初始化和代码块赋值的先后顺序完全取决于代码声明顺序）（九）final 关键字 final修饰的类不能被继承。 final修饰的方法不能被重写。 final修饰的变量不能修改。（常量） 修饰的属性变量：显式初始化、代码块中赋值、构造器中赋值。 修饰的局部变量：常量（多用作修饰方法形参） （十）abstract 关键字 修饰类 不能实例化。 修饰方法 只有声明，没有方法体。 需要子类重写其全部的抽象方法才能实例化。 不能修饰私有方法、静态方法、final方法、final类。（十一）抽象类的匿名子类相当于在创建子类对象时，将子类的抽象方法实现直接写在后面。（十二）接口 只能定义全局常量和抽象方法。 书写时可以省略关键字。 接口中不能定义构造器，接口不可以实例化。 用类来实现接口，可以实现多个接口。 class AA extends A implements Aable, Bable, Cable {} 接口之间可以继承，也可以多继承。 接口的匿名实现类与“抽象类的匿名子类”实现方法一致。 Java8新增定义静态方法、默认方法 接口中的静态方法只能通过接口来调用，实现类的对象无法调用。 接口中的默认方法可以通过实现类的对象来调用，可以在实现类中覆盖重写。 如果实现类的父类中的方法和接口中的默认方法重名且没有重写，则父类中的方法优先。 如果多继承的实现类多接口中有重名默认方法，且实现类没有重写，则接口冲突。 interface AA { public static void method1() {} // 静态方法 Java8 public default void method2() {} // 默认方法 Java8 public void method3(); // 抽象方法} class SubClass extends SuperClass implements CompareA, CompareB { public void method() {} public void myMethod() { method(); super.method(); CompareA.super.method(); CompareB.super.method(); }} （十三）内部类 成员内部类 可以调用外部类的成员。 可以被static修饰。 可以被权限修饰。 class Person { private String name; private int age; public void eat() {} class Bird { private String name; public void method() { System.out.println(); Person.this.eat(); // eat(); } public void test(String name) { System.out.println(name); System.out.println(this.name); System.out.println(Person.this.name); } } static class Dog {}} class Test { public static void main(String[] args) { Person.Dog dog = new Person.Dog(); Person p = new Person(); Person.Bird bird = p.new Bird(); }} 局部内部类 局部内部类使用的外部的局部变量默认为final。 五、异常处理（一）异常类别 编译时异常 IOException、ClassNotFoundException 运行时异常（RuntimeException） NullPointerException、ClassCastException、ArrayIndexOutOfBoundsException、NumberFormatException、InputMismatchException、ArithmeticException catch从上到小捕捉，只要被捕捉就不会继续。要求子类异常在父类异常上面。 try { // ...} catch (Exception e) { System.out.println(e.getMessage()); // 获取源异常信息 e.printStackTrace(); // 获取异常堆栈信息} finally { // 一定会被执行}throw new Exception(); // 手动抛出异常 finally块中的代码会抢在try和catch块中的return之前被执行。 子类重写的方法抛出的异常不能大于父类规定的抛出异常的类型。（二）自定义异常类 基于现有的异常结构，继承。 提供全局常量：serialVersionUID 提供重载的构造器。" }, { "title": "Spark数据库操作初步", "url": "/posts/spark-db/", "categories": "Distributed Computing", "tags": "Spark, Hadoop", "date": "2019-11-29 11:31:03 +0800", "snippet": " 操作系统：Windows 10 1909版本 IDE：IntelliJ IDEA Ultimate 2019.2.4版本 JDK：1.8.0_221 Hadoop：2.7.1 Spark：3.0.0-preview Scala：2.12.10 Maven：3.6.2 数据库：MySQL 8.0.18 一、准备工作在之前环境的基础上我们需要安装一个数据库，本文选取MySQL最新版本8.0.18。安装部署数据库的方法不再详述，各位可以参考MySQL 8.0.18安装教程(Windows 64位)。数据库完成部署之后可以在IntelliJ IDEA中用附带的插件进行连接，点击右侧栏中的Database，添加新的MySQL连接，设置界面如下。如果没有连接驱动，直接默认下载最新版的即可（虽然最新版的是8.0.15，但是实测可以连接8.0.18版本的MySQL数据库）。URL处一定要指定时区，否则会连接失败；另外注意输入正确的用户名和密码。此时点击测试连接应该会出现上图中绿色的对勾和相关信息，点击OK即可。然后在Database栏位中创建一个新的数据库，名为：hivemetastore。然后在其中创建相应的表并且导入一些示例数据。drop table if exists people;create table people( id serial primary key, name varchar(20), level smallint default 0);insert into people values (1, 'zhu', 4), (2, 'katus', 5), (3, 'li', 4);然后我们需要在pom.xml中补充新的依赖，即MySQL连接驱动相关依赖，更新之后的pom.xml如下。&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;zju&lt;/groupId&gt; &lt;artifactId&gt;WordCount&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;jdk.version&gt;1.8.0&lt;/jdk.version&gt; &lt;spark.version&gt;3.0.0-preview&lt;/spark.version&gt; &lt;scala.version&gt;2.12&lt;/scala.version&gt; &lt;mysql.version&gt;8.0.18&lt;/mysql.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;然后倒入相关依赖，完成即可。二、Spark连接数据库以下是一份连接hivemetastore数据库，读取people表中全部数据并显示在控制台的代码。import org.apache.spark.sql.SQLContext;import org.apache.spark.sql.SparkSession;import java.util.Properties;public class DbConnect { public static void main(String[] args) { SparkSession session = SparkSession.builder().getOrCreate(); SQLContext sqlContext = new SQLContext(session); // 数据库连接参数 String dbUrl = \"jdbc:mysql://localhost:3306/hivemetastore?serverTimezone=Asia/Shanghai\"; String tableName = \"people\"; Properties connectionProperties = new Properties(); connectionProperties.put(\"user\", \"root\"); connectionProperties.put(\"password\", \"xxxxx123\"); connectionProperties.put(\"driver\", \"com.mysql.cj.jdbc.Driver\"); // 读取hivemetastore数据库中的people表的全部内容 sqlContext.read().jdbc(dbUrl, tableName, connectionProperties).select(\"*\").show(); session.stop(); }}运行之后在控制台会有如下输出。（没有截取Spark集群的状态信息的控制台输出）对上面的代码进行简单的解释就是，SQLContext是所有的数据库操作的核心。SQLContext对象的read函数会返回一个DataFrameReader对象，DataFrameReader对象可以调用jdbc函数返回读取的数据库表，类型为Dataset，Dataset调用select可以筛选出一个至多个数据列（此处选择全部列其实是可以省略的，对结果没有影响），然后可以通过直接调用show函数直接以表格的形式直接输出在控制台。 Spark官方文档有一段对SQLContext类的描述：“As of Spark 2.0, this is replaced by SparkSession. However, we are keeping the class here for backward compatibility.”；也就是说，在Spark 2.0之后的版本可以直接用SparkSession替代SQLContext，SQLContext的保留只是为了向下兼容。实测也是可以替代的。三、简单的读取数据库操作为了体现出更多的数据库操作，将示例表格和数据进行复杂化，更新如下。drop table if exists people;create table people( id int primary key, name varchar(20), level smallint default 0, experience float);insert into people values (1, 'zhu', 4, 48.24), (2, 'katus', 5, 82.49), (3, 'li', 4, 56.34);drop table if exists device;create table device( id serial primary key, name varchar(20), price float default 0, ownerId int references people(id));insert into device values (1, 'computer', 6250.5, 2), (2, 'radio', 126.4, 1);查询经验值大于50的人的姓名（name）和级别（level）并输出到控制台。import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.SQLContext;import org.apache.spark.sql.Dataset;import java.util.Properties;public class DbConnect { public static void main(String[] args) { SparkSession session = SparkSession.builder().getOrCreate(); SQLContext sqlContext = new SQLContext(session); // 数据库连接参数 String dbUrl = \"jdbc:mysql://localhost:3306/hivemetastore?serverTimezone=Asia/Shanghai\"; String tableName = \"people\"; Properties connectionProperties = new Properties(); connectionProperties.put(\"user\", \"root\"); connectionProperties.put(\"password\", \"xxxxx123\"); connectionProperties.put(\"driver\", \"com.mysql.cj.jdbc.Driver\"); // 查询经验值大于50的人的姓名（name）和级别（level）并输出到控制台 Dataset people = sqlContext.read().jdbc(dbUrl, tableName, connectionProperties).select(\"*\"); people.select(\"name\", \"level\").filter(people.col(\"experience\").gt(50)).show(); session.stop(); }}输出结果如图。查询拥有ID为1的设备的人对应的编号（ID）、姓名（name）和经验值（experience）。import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.sql.Row;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.SQLContext;import org.apache.spark.sql.Dataset;import scala.Tuple2;import java.util.Properties;public class DbConnect { public static void main(String[] args) { SparkSession session = SparkSession.builder().getOrCreate(); SQLContext sqlContext = new SQLContext(session); // 数据库连接参数 String dbUrl = \"jdbc:mysql://localhost:3306/hivemetastore?serverTimezone=Asia/Shanghai\"; Properties connectionProperties = new Properties(); connectionProperties.put(\"user\", \"root\"); connectionProperties.put(\"password\", \"xxxxx123\"); connectionProperties.put(\"driver\", \"com.mysql.cj.jdbc.Driver\"); // 查询拥有ID为1的设备的人对应的编号（ID）、姓名（name）和经验值（experience） Dataset people = sqlContext.read().jdbc(dbUrl, \"people\", connectionProperties); Dataset device = sqlContext.read().jdbc(dbUrl, \"device\", connectionProperties); JavaPairRDD peopleRDD = people.toJavaRDD().mapToPair((PairFunction&lt;Row, Integer, Tuple2&lt;String, Double&gt;&gt;) row -&gt; { Integer personId = (Integer) row.get(0); String personName = (String) row.get(1); Double personExp = (Double) row.get(3); return new Tuple2&lt;&gt;(personId, new Tuple2&lt;&gt;(personName, personExp)); }); JavaPairRDD deviceRDD = device.toJavaRDD().mapToPair((PairFunction&lt;Row, Integer, Integer&gt;) row -&gt; { Integer deviceId = (Integer) row.get(0); Integer ownerId = (Integer) row.get(3); return new Tuple2&lt;&gt;(ownerId, deviceId); }); for (Tuple2&lt;Integer, Tuple2&lt;Tuple2&lt;String, Double&gt;, Integer&gt;&gt; item : (Iterable&lt;Tuple2&lt;Integer, Tuple2&lt;Tuple2&lt;String, Double&gt;, Integer&gt;&gt;&gt;) peopleRDD.join(deviceRDD).collect()) { if (item._2()._2() == 1) { System.out.println(\"id: \" + item._1().toString() + \" ; name: \" + item._2()._1()._1() + \" ; exp: \" + item._2()._1()._2().toString()); } } session.stop(); }}运行结果如图。最后一个查询相对而言还是十分复杂的，此处涉及到两个表格的自然连接操作，直接对单一表格的SQL查询是难以实现的，因此将每个表格的查询结果转化为RDD，然后使用RDD中的join操作，最后遍历结果，筛选出符合条件的记录。但是这种实现从代码角度考虑前面的操作虽然有分布式计算的加速，但是最后对于join结果的遍历是完全依靠Driver程序实现的，这个运算效率会大大下降，因此可以考虑在join之前就对表格记录进行尽可能地筛选，减少最后Driver程序的运算量。（当然这个只是个人的实现，可能还存在更加简单高效的实现方法，待进一步研究）四、简单的写入数据库操作实现向数据库表people中插入两条数据，初始时数据库people表记录如下。然后新创建一个Java类，填充如下代码。import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.RowFactory;import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructType;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.Properties;public class DbWrite { public static void main(String[] args) { SparkSession session = SparkSession.builder().getOrCreate(); JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(session.sparkContext()); // 数据库连接参数 String dbUrl = \"jdbc:mysql://localhost:3306/hivemetastore?serverTimezone=Asia/Shanghai\"; Properties connectionProperties = new Properties(); connectionProperties.put(\"user\", \"root\"); connectionProperties.put(\"password\", \"xxxxx123\"); connectionProperties.put(\"driver\", \"com.mysql.cj.jdbc.Driver\"); // 构建新的数据记录RDD JavaRDD&lt;String&gt; personData = sparkContext.parallelize(Arrays.asList(\"4,jiang,5,79.46\", \"5,wan,4,63.49\")); // 将RDD成员（String）分割成四个field JavaRDD&lt;Row&gt; personsRDD = personData.map((Function&lt;String, Row&gt;) line -&gt; { String[] splited = line.split(\",\"); return RowFactory.create(Integer.valueOf(splited[0]), splited[1], Integer.valueOf(splited[2]), Double.valueOf(splited[3])); }); List structFields = new ArrayList(); structFields.add(DataTypes.createStructField(\"id\", DataTypes.IntegerType, false)); structFields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, true)); structFields.add(DataTypes.createStructField(\"level\", DataTypes.IntegerType, true)); structFields.add(DataTypes.createStructField(\"experience\", DataTypes.DoubleType, true)); // 构建StructType（类似创建一个表格结构的描述） StructType structType = DataTypes.createStructType(structFields); // 构建数据集（用于和数据库对接） Dataset personsDF = session.createDataFrame(personsRDD, structType); // 以追加的形式写入数据库的people personsDF.write().mode(\"append\").jdbc(dbUrl, \"people\", connectionProperties); session.stop(); }}运行之后数据库中数据发生了如下变化。" }, { "title": "Spark RDD初探", "url": "/posts/spark-rdd/", "categories": "Distributed Computing", "tags": "Spark, Hadoop", "date": "2019-11-27 15:08:29 +0800", "snippet": " 弹性分布式数据集（Resilient Distributed Dataset，RDD）是Spark中的核心概念，基本上所有的Spark运算操作对象都是RDD，我们今天就来简单认识一下这个RDD。 说明：由于本文的开发基于Java，因此所有的观点都是基于Java的。一、RDD的创建RDD是一种数据集，初步我们可以将其想象成一个数组类似的数据结构，先不去管实际的存储结构。RDD的创建方式可以划分成两大类。（一）从Driver程序的数据集生成RDD直说就是程序本身数据生成RDD，不是从外部导入的数据。一般是通过SparkContext对象（实际为JavaSparkContext）的parallelize方法来创建RDD。SparkSession session = SparkSession.builder().getOrCreate();JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(session.sparkContext());List&lt;Integer&gt; list1 = Arrays.asList(1, 2, 3, 4, 5);List&lt;String&gt; list2 = Arrays.asList(\"hello\", \"RDD\", \"Spark\");JavaRDD&lt;Integer&gt; myRDD1 = sparkContext.parallelize(list1);JavaRDD&lt;String&gt; myRDD2 = sparkContext.parallelize(list2);上述代码中myRDD1和myRDD2均为通过程序的数据集生成的RDD。 Java开发中RDD（实际为JavaRDD）只能通过List数据类型来生成。（二）从外部数据集生成RDD从外部数据集加载的相关方法有很多，包括（可能不限于）如下列举的方法。 textFile方法，从文本文件加载。 JavaRDD&lt;Integer&gt; myRDD = sparkContext.textFile(inputFile); hadoopFile方法，从Hadoop文件加载。 sequenceFile方法。 objectFile方法，读取序列化文件。 binaryFiles方法，以二进制格式直接读取Hadoop MapReduce计算的结果文件。 hadoopRDD方法，读取HBase文件。 二、RDD 操作RDD的操作分为两大类，分别是Transformation（转换）和Action（动作）。Spark进行Transformation时采用lazy模式，即计算不是立刻执行，只有当Action操作触发时才会进行启动运算。 Transformation：由一个RDD生成另一个RDD的过程。 map(function) 对RDD中的每个元素进行function操作，生成新元素构成的新RDD返回。 filter(function) 对RDD中的元素进行过滤，如果调用函数返回true则保留，返回过滤后的RDD。 flatMap(function) 与map类似，但是每个元素调用之后可能会产生0至多个元素，将这些所有的元素全部扁平化构成一个新的RDD，要求function的返回类型为Seq类型。 mapPartitions(function) 与map类似，但是function的作用对象是一整个分区，即Iterator -&gt; Iterator。 mapPartitionsWithIndex(function) 与mapPartitions类似，但是function的输入会多一个整形变量，表示分区编号，即Int, Iterator -&gt; Iterator。 sample(withReplacement, fraction, seed) 对RDD中的元素进行抽样，withReplacement（布尔值）表示是否放回，fraction（浮点型）表示抽样比例，seed表示随机数种子。 union(otherDataSet) 合并两个RDD，不去重，要求类型完全一致。 intersection(otherDataSet) distinct([numTasks]) 对RDD进行去重操作。 groupByKey([numTasks]) 对键值对形式的RDD进行按照键分组的操作，返回RDD形式为&lt;key, Iterable&gt;。 reduceByKey(function, [numTasks]) 对键值对形式的RDD进行按照键聚合的操作，function必须为V, V -&gt; V的形式。 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) sortByKey([ascending], [numTasks]) 对键值对形式的RDD进行按照键排序的操作。 join(otherDataSet, [numTasks]) 对(K, V)和(K, W)进行join操作得到(K, (V, W))，其他连接操作包括leftOuterJoin、rightOuterJoin和fullOuterJoin。 cogroup(otherDataSet, [numTasks]) cartesian(otherDataSet) pipe(command, [envVars]) coalesce(numPartitions) repartition(numPartitions) repartitionAndSortWithinPartitions(partitioner) Action：返回结果到Driver程序中，常表示运算完成。 reduce(function) 对RDD进行reduce操作最终返回一个值。 collect() 将RDD返回到Driver程序，类型为Array。 count() 返回RDD的元素数量。 first() 返回RDD的第一个元素，等价于take(1)。 takeSample(withReplacement, num, [seed]) take(n) 返回RDD的前n个元素。 takeOrdered(n, [ordering]) saveAsTextFile(path) 将RDD转化为文本格式存储在path下。 saveAsSequenceFile(path) 与saveAsTextFile类似，但是存储格式为SequenceFile。 saveAsObjectFile(path) countByKey() 对键值对形式的RDD返回key的数量。 foreach(function) 对RDD中的每个成员执行function，对RDD没有影响。（常用于更新计数器之类的操作） 下面的代码是一段从文本文件读入然后统计每个单词数量并写出到文件的代码。import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.SparkSession;import scala.Tuple2;import java.util.ArrayList;import java.util.List;public class WordCount { public static void main(String[] args) { String inputFile = \"D:\\\\Data\\\\words.txt\"; String outputFile = \"D:\\\\Data\\\\result\"; SparkSession session = SparkSession.builder().getOrCreate(); JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(session.sparkContext()); sparkContext.textFile(inputFile) // 对于每个段落，进行split操作，然后将单词与1配对，扁平化，最终形成JavaPairRDD .flatMapToPair(s -&gt; { String[] words = s.split(\"\\\\s\"); List&lt;Tuple2&lt;String, Integer&gt;&gt; r = new ArrayList&lt;&gt;(); for (String word : words) { r.add(new Tuple2&lt;&gt;(word, 1)); } return r.iterator(); }) // 对于每个元组对，按照key合并，后面的数字累加 .reduceByKey(Integer::sum) // 将结果保存为文本文件 .saveAsTextFile(outputFile); session.stop(); }}如果我想把结果输出显示在控制台，而不输出到文件。import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.SparkSession;import scala.Tuple2;import java.util.ArrayList;import java.util.List;public class WordCount { public static void main(String[] args) { String inputFile = \"D:\\\\Data\\\\words.txt\"; SparkSession session = SparkSession.builder().getOrCreate(); JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(session.sparkContext()); sparkContext.textFile(inputFile) // 对于每个段落，进行split操作，然后将单词与1配对，扁平化，最终形成JavaPairRDD .flatMapToPair(section -&gt; { String[] words = section.split(\"\\\\s\"); List&lt;Tuple2&lt;String, Integer&gt;&gt; list = new ArrayList&lt;&gt;(); for (String word : words) { list.add(new Tuple2&lt;&gt;(word, 1)); } return list.iterator(); }) // 对于每个元组对，按照key合并，后面的数字累加 .reduceByKey(Integer::sum) // 将RDD返回到Driver程序 .collect() // 对于RDD中的每个成员调用输出函数 .forEach(WordCount::print1); session.stop(); } // 自定义的静态输出函数 private static void print1(Tuple2&lt;String, Integer&gt; tu) { System.out.println(tu.toString()); }}上述RDD操作都是用了函数作为参数，事实上，Spark严重依赖于传递函数类型的参数，常见的RDD操作都需要提供一个函数作为操作方法的参数。Spark运算的核心是RDD，而RDD的运算又是分布式的，因此虽然代码看上去都是在本地运行，但是实际上都不在本地计算。中心结点会将计算所依赖的全部变量、方法打包在一起序列化发送到各个结点，各个结点各自进行反序列化，然后进行运算，最终将运算结果发送到中心结点。在这种机制下，RDD操作是绝对不能嵌套调用的，只能进行顺序操作。在各种运算场景下，有部分场景要求RDD是键值对的形式，即&lt;key, value&gt;的形式。这种形式的RDD为PairRDD，经常进行shuffle操作，比如按key进行分组或者聚合。三、RDD持久化和共享变量（一）RDD持久化通常来说，如果一个中间结果RDD被多次利用，将其存入缓存可以极大程度上提高运算效率，在开发中RDD持久化通常是通过persist方法来实现的。sparkContext.textFile(inputFile).persist(StorageLevel.MEMORY_ONLY());这种持久化有多种类型，包括如下几种。 StorageLevel 意义 MEMORY_ONLY 仅持久化到内存 MEMORY_AND_DISK 持久化到内存，不够时使用磁盘 MEMORY_ONLY_SER 序列化数据后仅持久化到内存 MEMORY_AND_DISK_SER 序列化数据后持久化到内存，不够时使用磁盘 DISK_ONLY 仅持久化到磁盘 另外可以使用unpersist函数进行去持久化。（二）共享变量1.广播变量在每个结点上都有一份缓存的变量，并且是只读的，没有修改其值的意义。Broadcast&lt;Integer&gt; broadcastVar1 = sparkContext.broadcast(1);Broadcast&lt;ArrayList&gt; broadcastVar2 = sparkContext.broadcast(Arrays.asList(\"first\", \"second\", \"third\"));2.计数器计数器只能增加，通常用来计数或者求和，Spark在Java中没有直接的计数器支持，需要自己重写。对于Spark RDD的更进一步的学习，我们放在以后进行。" }, { "title": "Spark开发环境搭建", "url": "/posts/spark-environment/", "categories": "Distributed Computing", "tags": "Spark, Hadoop", "date": "2019-11-26 11:56:47 +0800", "snippet": " 前言：本人水平有限，目前在前辈的指导下进行Spark开发的自学，在此整理出自学笔记，主要是巩固一下学习的内容，如果本文内容能对各位读者有所启发，我将十分高兴。另外由于个人水平有限，所写内容难免有疏漏之处，欢迎各位批评指出。 环境： 操作系统：Windows 10 1909版本 IDE：IntelliJ IDEA Ultimate 2019.2.4版本 JDK：1.8.0_221 Hadoop：2.7.1 Spark：3.0.0-preview Scala：2.12.10 Maven：3.6.2 一、Java环境搭建安装java环境需要安装部署JDK和JRE，本次选择的JDK版本是1.8。前往官网即可下载，官网链接：JDK下载地址。我们选择64位Windows版本下载，然后进行安装。安装完成之后进行系统变量的配置，需要配置如下系统变量。（请根据自己的安装目录进行设置） JAVA_HOME：C:\\Program Files\\Java\\jdk1.8.0_221 CLASSPATH：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; Path：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin完成之后在命令行分别输入java、javac命令均有相应结果，说明配置正确。二、安装IDE本文选用的IDE为JetBrains套件中的IntelliJ IDEA，可以前往JetBrains官网下载安装，具体安装方式不再演示。三、Hadoop安装本文下载的Hadoop版本为2.7.1，前往Hadoop镜像网站下载对应的Hadoop包。在Windows环境下部署Hadoop环境与Linux不同，需要额外的文件，不单单是解压就可以了。额外的文件包括hadoop.dll和winutils.exe，这两个文件可以在4ttty的winutils GitHub项目下找到，复制这两个文件到Hadoop文件夹下的bin目录下。 实际在配置的时候如果只添加上述两个文件还是会导致Spark项目运行报错，说winutils.exe与你运行的Windows版本不兼容；后来我经过大量尝试发现需要把上述的GitHub工程中对应Hadoop版本下的bin文件夹下的全部拷贝到本机上的Hadoop/bin文件夹下才能正常。完成后配置系统环境变量，如下。（请根据自己的安装目录进行设置） HADOOP_HOME：D:\\Compilers\\hadoop-2.7.1 Path：%HADOOP_HOME%\\bin四、Spark安装在Spark官网下载合适版本的Spark，需要注意版本需要契合之前安装的Hadoop版本，本文就选用3.0.0-preview版本。和Hadoop的安装类似，解压，修改系统环境变量如下。（请根据自己的安装目录进行设置） SPARK_HOME：D:\\Compilers\\spark-3.0.0-preview-bin-hadoop2.7 Path：%SPARK_HOME%\\bin五、Scala语言安装前往Scala官方网站选择相应版本的Scala版本进行安装，注意此处安装的版本需要和选用的Spark版本对应的Scala版本相一致才行，例如本文选用的Spark版本为3.0.0-preview（目前最新），其对应的适用Scala版本为2.12.10。查询这种对应关系可以通过查看Spark中的jar包中的scala-compiler-2.XX.XX.jar，最后面的数字即为对应的Scala版本。本文中的对应jar包为scala-compiler-2.12.10，即Scala版本为2.12.10。安装过程保持默认即可。完成后修改系统变量如下。（请根据自己的安装目录进行设置） SCALA_HOME：C:\\Program Files (x86)\\scala Path：%SCALA_HOME%\\bin六、安装maven前往maven官网下载maven的二进制ZIP包，在特定目录解压即可。七、工程项目环境配置（一）IDE配置打开IntelliJ IDEA，第一次使用需要激活、设定UI、选用下载插件，统统默认完成之后打开如下界面。接下来需要依次设置配置下的Plugins、Structure for New Projects、Settings选项。首先是Plugins，安装Scala插件，完成后点击OK。然后设置Structure for New Projects，Project选项卡，点击New按钮，点击JDK选项，选择刚才安装的JDK路径，确认。Global Libraries选项卡，点击加号，选择Scala SDK，添加刚刚在系统中安装的Scala，点击OK。最后设置Settings，选择Build, Execution, Deployment下的Build Tools下的Maven选项卡，设置右侧Maven home directory为刚刚maven下载的解压目录，其中User settings file和Local repository可以通过勾选Override选项更换位置。此处建议将Local repository设置到其他位置。更改repository默认位置可以通过在maven配置文件settings.xml的settings标签下增加如下代码来实现。(标签中的值为自定义设置的repository目录)&lt;localRepository&gt;D:\\Compilers\\maven-repository\\repository&lt;/localRepository&gt;如果在一会下载依赖jar包的时候如果速度过慢，可以使用国内镜像，方法是在maven配置文件settings.xml的settings标签下的mirrors标签下增加如下代码（以阿里云镜像为例）。&lt;!-- 阿里云镜像 --&gt;&lt;mirror&gt;\t&lt;id&gt;nexus-aliyun&lt;/id&gt;\t&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;\t&lt;name&gt;Nexus aliyun&lt;/name&gt;\t&lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;点击Compiler下的Java Compiler选项卡，设置合适的编译器版本，此处选择8。（截图中WordCount会在后续新建项目之后出现，后续也可以通过菜单栏再次修改Settings） 此处设置的编译器版本取决于你的Spark支持的版本、JDK版本和你的代码中所用到的Java特性。设置完成点击OK即可。（二）创建Maven项目点击Create New Project，在左侧选择Maven，右侧JDK版本选择1.8，完成后点击Next。设置GroupId和ArtifactId，没有什么具体要求，建议ArtifactId与项目名一致，完成后点击Next。设置项目名称，项目保存路径，完成后点击Finish。（三）配置Maven依赖在新项目根目录下的pom.xml修改为如下代码，请注意配置时GroupId和ArtifactId请以自己的为准。&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;zju&lt;/groupId&gt; &lt;artifactId&gt;WordCount&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;jdk.version&gt;1.8.0&lt;/jdk.version&gt; &lt;spark.version&gt;3.0.0-preview&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0-preview&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0-preview&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0-preview&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;配置Maven依赖时，如果遇到不清楚的依赖写法，可以通过在Maven存储库网站中进行查询。然后在项目目录下的pom.xml文件上右键，点击Maven下的Reimport，重新导入依赖。（或者在右下角的提示中选择导入修改）等待下载完成即可。（四）示例程序测试在新建的工程中新建WordCount类，然后增加如下代码。import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.SparkSession;import scala.Tuple2;import java.util.ArrayList;import java.util.List;public class WordCount { public static void main(String[] args) { String inputFile = \"D:\\\\words.txt\"; String outputFile = \"D:\\\\result\"; SparkSession session = SparkSession.builder().getOrCreate(); JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(session.sparkContext()); sparkContext.textFile(inputFile).flatMapToPair(s -&gt; { String[] words = s.split(\"\\\\s\"); List&lt;Tuple2&lt;String, Integer&gt;&gt; r = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); for (String word : words) { r.add(new Tuple2&lt;String, Integer&gt;(word, 1)); } return r.iterator(); }).reduceByKey((i, j) -&gt; i + j).saveAsTextFile(outputFile); session.stop(); }}在右上角配置java运行环境，配置如下。然后点击运行，正常运行说明配置成功。最后查看运行结果如下。" } ]
